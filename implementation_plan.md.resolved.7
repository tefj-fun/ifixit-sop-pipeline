# SOP Pipeline & Training Architecture Design

## Goal Description
Design a complete pipeline for an automated Standard Operating Procedure (SOP) system. The system ingests raw manuals, parses them into structured logic/classes using an **SOP Agent**, assists human annotation using an **Annotation Copilot**, and verifies steps at runtime using a **Logic Agent**.

## User Review Required
> [!IMPORTANT]
> This design relies heavily on iFixit data as the "Gold Standard" for training. The downloader must be robust enough to fetch diverse examples (tools, parts, logic types).

## 1. Pipeline Architecture

```mermaid
graph TD
    UserUpload["User Uploads Manual"] --> SOP_Agent["SOP Agent (LLM)"]
    SOP_Agent --> StructuredJSON["Structured JSON (Steps, Logic, Classes)"]
    
    subgraph "Annotation Phase"
        StructuredJSON --> AnnotationTool["Annotation Tool"]
        AnnotationCopilot["Annotation Copilot (VLM/YOLO)"] -->|"Suggests BBoxes"| AnnotationTool
        User["User"] -->|"Verifies/Corrects"| AnnotationTool
        AnnotationTool --> TrainedModel["Train Dedicated Vision Model"]
    end
    
    subgraph "Runtime Phase"
        LiveFeed["Live Camera Feed"] --> TrainedModel
        TrainedModel --> Detections["Detections (Class, Conf, Box)"]
        StructuredJSON --> LogicAgent["Logic Agent"]
        Detections --> LogicAgent
        LogicAgent --> Result{"OK / NG / Next Step"}
    end
```

## 2. Agent Design & Training Strategy

### A. SOP Agent (The Parser)
*   **Role**: Converts raw manual text -> Structured JSON (Schema defined below).
*   **Training Data Source**: iFixit Guides.
    *   *Input*: Raw HTML/Text of a guide step.
    *   *Output*: The structured JSON we designed (with `lines`, `bullets` mapped to logic).
*   **Training Method**: Fine-tuning (LoRA/QLoRA) on a base LLM (e.g., Llama 3, Mistral).
    *   *Prompt*: "Extract the step instructions, tools, and logic from the following text into JSON."

### B. Annotation Copilot (The Assistant)
*   **Role**: Pre-annotates images to save user time.
*   **Training Data Source**: iFixit Images + Weak Labels.
    *   *Weak Labels*: The colored bullets in the text ("Red") map to colored circles in the images.
*   **Training Method**:
    *   **Option 1 (VLM)**: Fine-tune a VLM (e.g., LLaVA, PaliGemma) to output bounding boxes for named objects.
    *   **Option 2 (Zero-Shot)**: Use Grounding DINO with the class names extracted by the SOP Agent. No training required, just evaluation.

### C. Dedicated Vision Model (The Runtime Engine)
*   **Role**: Fast, accurate detection at runtime (YOLO/EfficientDet).
*   **Training Strategy**:
    *   **Step 1: Pre-training (Transfer Learning)**: Train on the **ENTIRE** iFixit dataset (all 100k+ guides, every category from "Mac" to "Toaster" to "Car"). No filtering. This maximizes the model's exposure to diverse objects.
    *   **Step 2: Fine-tuning**: Train on the user's specific, high-quality annotated data (from the Annotation Tool).

### D. Logic Agent (The Judge)
*   **Role**: Evaluates if a step is complete based on vision telemetry.
*   **Design**: A deterministic code engine (Python) or a small specialized LLM.
*   **Logic Primitives**:
    *   `Count(Class) == N` (e.g., "4 screws removed")
    *   `Absence(Class)` (e.g., "Battery is gone")
    *   `State(Class) == X` (e.g., "Connector is disconnected")

## 3. Data Schema (The Contract)

The schema serves as the bridge between all agents.

```json
{
  "id": "guide_123",
  "steps": [
    {
      "order": 1,
      "instruction": "Remove the four 4mm Phillips screws.",
      "logic": {
        "type": "count_check",
        "target_class": "Phillips #00 Screw",
        "operator": "==",
        "value": 4,
        "state": "removed"
      },
      "classes_to_annotate": [
        {"name": "Phillips #00 Screw", "color_marker": "red"}
      ]
    }
  ]
}
```

## 4. Implementation Steps (This Task)

### [NEW] `ifixit_downloader.py`
*   **Purpose**: The engine to fuel the training of Agent A and Agent B.
*   **Features**:
    *   Downloads Guides -> Saves as `raw_text` (for SOP Agent input) and `structured_json` (for SOP Agent label).
    *   Downloads Images -> Saves with metadata about bullet colors (for Copilot weak supervision).

### [NEW] `train_sop_agent_dataset_gen.py`
*   **Purpose**: Converts downloaded iFixit data into a HuggingFace-ready dataset (JSONL) for fine-tuning the SOP Agent.

## Verification Plan
1.  **Run Downloader**: Fetch 50 guides.
2.  **Verify Dataset**: Check that the generated JSONL contains valid Input/Output pairs for the SOP Agent.
3.  **Verify Logic Extraction**: Manually check if the schema captures the "4 screws" logic correctly from the raw text.
